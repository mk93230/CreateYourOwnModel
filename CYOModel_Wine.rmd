---
title: "Multi Class Classification for Wine Dataset"
author: "Mahalingam Kamalahasan"
date: "6/12/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Multi Class Classification for Wine Dataset

Overview:

We would like to identify the wine types (the cultivars) based on the 13 chemical
components. The 13 components are 
1. Alcohol
2. Malic 
3. Ash           
4. Alcalinity 
5. Magnesium
6. Phenols 
7. Flavanoids
8. Nonflavanoids   
9. Proanthocyanins
10.Color
11.Hue
12.Dilution
13.Proline

The net output is the wine type that are classified as 1,2 or 3

The goal is to build a model that would predict the wine type with a minium of 68% accuracy . Which is a pretty good estimate for this type of data. I have used k nearest neighbor model to build the model and achieve the accuracy of 68% or above.

About the data set:
The rattle package contains the data for wines grown in specific area of Italy
The wine data was obtained from UCI Machine Learning Repository.

```{r echo=FALSE}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")

# If caret package does not exist, load from the following repo and install it
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")


```

You can obtain the data set by installing the "rattle" package

```{r echo=TRUE}
if(!require(rattle)) install.packages("rattle")

# The following command will list all the datasets in a package, we are interested
# in the wine dataset. 
data(package="rattle") 

#Loading the wine dataset
data("wine")


```

## Created Data Partition so that we can have train dataset and test dataset. We are randomly picking 90% of the data to train and we will use the 10% of the data to validate our model. The code is as below

```{r echo=TRUE}
set.seed(66)
wine_test_index <- createDataPartition(y = wine$Type, times = 1, p = 0.1, list = FALSE)

############################ Training set #####################################
wine_train_set <- wine[-wine_test_index,]
# Store train input features in a separate variable
train_input_features <- wine_train_set %>% select(-Type)

# Store train output in a separate variable
train_output <- wine_train_set %>% select(Type) %>% .$Type

############################ Test set #####################################
wine_test_set <- wine[wine_test_index,]
# Store test input features in a separate variable
test_input_features <- wine_test_set %>% select(-Type)

# Store test output in a separate variable
test_output <- wine_test_set %>% select(Type) %>% .$Type
```

## Method and Analysis

To build the K Nearest Neighbor model, I have decided to train the model using some tuning parameters. I have decided to use cross validation with 5 resampling with 90% of the data. I then decided to train the model with k value of 1,3,4,5 and 7. This is to determine the ideal k value that has relatively good accuracy.
The plan is then to use the k value to create the final model. AS you can see from the plot knn=3 seems to produce an acceptable accuracy to build our model

```{r train_knn, echo=FALSE}
cv_no <- 5
control <- trainControl(method="cv", number=cv_no, p=0.9)

train_knn <- train(wine_train_set,train_output,method="knn", tuneGrid = data.frame(k=c(1,3,4,5,7)),trControl = control )
ggplot(train_knn)
```

## Method and Analysis - Contd
From the graph above we will use k value 3 to run our final model

```{r echo=TRUE}
# Now lets create a model with the ideal K 
knn_fit <- knn3(wine_train_set,train_output,k=3)

# now lets run the knn_fit model against the test data to get the predicted Y
y_hat_knn <- predict(knn_fit,wine_test_set,type="class")
```

